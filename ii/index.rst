
*****
chap1
*****

Immutable Infrastructureの最適解を探る
======================================

2014年のインフラ界隈についてまとめます。


出発点とオチ（あとで消す
-------------------------

* この章はいわゆる格子です
* Immutable Infrastructure って言うけど、どうなの最近

  * Immutable Infrastructureとは？

* chefつらぽよなエンジニアが増えている話をきいたりしてつらぽよ

  * どのへんつらぽよ？

* 業務で本当に使えてる？

  * 結局どうなるのよ、自動化して。恩恵はあるの？

* serfとかdokkerとかansibleとかいうけど実際使えてる？
* それじゃあそれぞれ解説しましょうか

  * と思ったけどテスト大事だよね！serverspecのお話からやっていく

    * それから、chefやansibleをプラがブルに使っていく話

  * chefつらぽよ。chefについてはvol.4でやったのでそっちを参照

    * chef辛かったのでrpm化してみたけどphpのrpmを作るのがつらぽよ
    * プロジェクトごとにconfigureオプション違うし、コマンドラインでやったほうが早げ

  * ansibleどう？

    * 使い方を解説

  * Vagrantとか使ったことないけど実際どうなの？どういう環境で動作するの？どの辺がうれしい？
  * hostsの書き換えどうしてる？
  * 監視ツールに自動で突っ込んでほしいよねー
  * CI as a Serviceが現段階でのゴールのような気がする

    * 自動化最高！ヒャッハー!!
    * IT全般籐製（ウッ頭が

* 業務に応じた感じでもろもろの技術をチョイスして組み合わせて使えるといいよね！（（結論

  * 多分このへんが無難な結論だと思う

* 読者層

  * インフラ屋
  * コーダー(プログラマー
  * あらゆるものを自動化したい人

* vagrantでコード一式を構築する -> テストをする
* chefやansibleでサーバを構築する -> テストをする(serverspec)

* Immutable Infraっていうけど、結果的にオンデマンドサーバ構築って方が近い気がする


この記事について
---------------

Immutable Infrastrucureの思想と、それを取り巻く技術を解説します。


Immutable Infrastructure とは
-----------------------------

いままでの環境をぶっ壊して、新しく作りなおすことです。目的は、本番稼働しているサーバのデプロイを失敗なく行うことにあります。

.. 若干乱暴な言い方だけど概ね間違ってない認識

Immutable Infrastrucure(以下、IIと略します)は、今年のインフラ業界のバズワードと言っても過言ではないでしょう。直訳すると、「不変なインフラ」となります。
つまり、一度デプロイしたサーバには変更を加えないということを意味しています。
もし、アプリケーションに変更を加えたい場合は、再度サーバを構築すればいいのです。そして以前デプロイしたサーバは、綺麗さっぱり捨てます [#iidi]_。

.. [#iidi] 綺麗さっぱり捨てることからDisposal Componentという名前が適切という話もあります


発端
^^^^^

Kief Morris氏 [#iikief]_ が2013年6月に「ImmutableServer」 [#iiims]_ というエントリを上げ、その10日後、CHad Fowler氏 [#iichad]_ が「サーバを捨てて、コードを焼き付けろ！」(Trash Your Servers and Burn Your Code) [#iitys]_ というエントリを上げています [#iihottan]_ 。
「サーバを捨てて、コードを焼き付けろ！」に、興味深い示唆があるのでJunichi Niino氏の邦訳 [#iihottan]_ から引用してみましょう。

:: 
  
  開発者として、あるいはしばしばシステム管理をする者として、これまで経験したもっとも恐ろしいものの1つは、長年にわたり稼働し続けてなんどもシステムやアプリケーションのアップグレードを繰り返してきたサーバだ。
  なぜか。その理由は、古いシステムはつぎはぎのような処置がされているに違いないからだ。障害が起きたときに一時しのぎのハックで対処され、コンフィグファイルをちょこっと直してやり過ごしてしまう。「あとでChefの方に反映しておくよ」とそのときは言うけれど、炎上したシステムの対処に疲れて一眠りしたあとでは、そんなことは忘れてしまうだろう。
  予想もしないところでcronのジョブが走り始めて、よく分からないけれどなにか大事な処理をしていて、そのことを知っているのは関係者のうちの1人だけとか。通常のソースコード管理システムを使わずにコードが直接書き換えられているとか。システムがどんどん扱いにくくなっていき、手作業でしかデプロイできなくなるとか。初期化スクリプトがもはや、思いも付かないような例外的な操作をしなければ動かなくなっているとか。
  もちろんOSは（きちんと管理されているならば）なんども適切にパッチが当て続けられているだろうが、そこには（訳注：やがて秩序が失われていくという）エントロピーの法則が忍び込んでくるものだし、適切に管理されていないとすればいちどもパッチは当てられず、もしこれからパッチを当てようものならなにが起きるか分からない。
  私たちはこの問題を解決するために何年ものあいだ、チームポリシーの策定から自動化までさまざまな方法を試してきた。そしていま試している新しい方法が「Immutable Deployments」（イミュータブル・デプロイメント）だ。

.. [#iikief] http://kief.com/, https://twitter.com/kief
.. [#iiims] http://martinfowler.com/bliki/ImmutableServer.html
.. [#iichad] https://twitter.com/chadfowler
.. [#iitys] 邦題は@naoya氏の「Immutable Infrastructure Conference #1」の発言から引用。「Trash Your Servers and Burn Your Code: Immutable Infrastructure and Disposable Components」http://chadfowler.com/blog/2013/06/23/immutable-deployments/
.. [#iihottan] このへんの流れは、 Junichi Niino氏の「『Immutable Infrastructure（イミュータブルインフラストラクチャ）と捨ててしまえるコンポーネント』 チャド・ファウラー氏」http://www.publickey1.jp/blog/14/immutable_infrastructure.html　を参考にしました。っていうかほぼそのまま


II以前の世界
^^^^^^^^^^^

ここで、インフラあるあるをご覧ください：

* 「あ、明日までに50台構築して。インストール手順は散らばってるからよろしく」「えっ？インストール手順作るの？？」
  * そして、次の日。構築はできたものの、職人の手によるバラツキが・・・
    * セッション数が多いときパフォーマンスがでないぞ？あ、sysctl変更するの忘れてた・・・ポートが枯渇してた・・・
* 別の日、メンテナンスのためサーバを再起動したらアプリケーションが動かなくなりました
* とある日、サーバが物理的に古くなったので新規に構築しようとしたらどこにもドキュメントがなくて、まずは何がインストールされているのか調べる羽目になりました
* ``$ crontab -r`` あ！やっちゃった！！戻さないと。。。バックアップがない！！！
* 「デプロイ職人」という肩書 (察し

繰り返される変更の結果、秘伝のタレが詰まったサーバと化していました [#iinao]_ 。

.. [#iinao] なお、これらはすべてフィクションです。現実に起こった事態とは一切関係はありません

特に本番環境は一つしかなく、プログラムをデプロイしたその瞬間、「503 Service Temporarily Unavailable」の文字が出現。
目の前がまっしろわーるど [#iimashiro]_ に遭遇した人は私だけではないでしょう [#iitaisho]_ 。
原因は、開発環境と本番環境の違いや、デプロイ職人の人為的ミス、複雑なデプロイの伝達ミス、設定値の変更し忘れ、など多岐にわたります。

.. [#iidep] 現在進行形でそういう運用を行っているところがあると思いますが...
.. [#iimashiro] TVアニメ「未確認で進行形」エンディングテーマ / iTunesでも配信しています
.. [#iitaisho] そう、この記事の読者対象はそういう経験をしたことがあるあなたです

本番環境とは離れたところでは、自分のマシンのVM上の開発環境でもろもろやった結果、どうもなにかがおかしい。
やっぱり最初の状態に戻したいというとき、いちから構築することが往々にしてあり、そこに時間がかかってしまう場合があります。


背景
^^^^^

「環境をぶっ壊して、新しく作りなおす」ことが簡単にできる技術が現れたのが、このIIが生まれた背景にあります。

* コードの管理はGit(と、pull request)
* コードのテストにvagrantやdocker、jenkins
* サーバの構築手順はpuppetやchef、ansible
* Amazon Web Service(AWS)といった仮想環境

こういった技術が2014年になってひと通り揃ってきました。


DevOps
^^^^^^^

ここからIIが生まれた理由について、寄り道をします。知ってるよ！ということであれば次の章へ飛んでください。

さて、DevOpsとは、開発（Development）と運用（Operations）のそれぞれの頭文字を取ったものです。悲しいかな、開発と運用は、しばしば対立します。
往々にして、運用は複数のシステムのサーバの面倒をみています。開発者は、問題が見つかったら本番環境でのログが見たいと思います [#iidevlog]_ 。
そのとき運用は、ほかの開発チームからの対応をしており、すぐには対応できないことが多々あります [#iidevops]_ 。
こういったことが積み重なり、開発者はすぐに見たいログが見えない、運用者は複数の開発者からのログ欲しい依頼キューが溜まっていきます [#iidevopsref]_ 。

.. [#iidevlog] 本番のログは秘密がいっぱいで直接見ることができない場合があります
.. [#iidevops] いやーあるんですよねこういう状況。最盛期だと本番へのデプロイを3つ並行しつつ、ログ欲しいよ依頼に対応してたり。え？もちろん聞いた話ですよ？？
.. [#iidevopsref] さらなるDevOpsについては http://www.atmarkit.co.jp/ait/articles/1307/02/news002.html

この状況を打破するために、自動化を図ります。


継続的デリバリー
^^^^^^^^^^^^^^^

継続的デリバリーを調べると、「継続的デリバリー 信頼できるソフトウェアリリースのためのビルド・テスト・デプロイメントの自動化」 [#iikz]_ という本がすぐに見つかります。
この本には、ソフトウエアをユーザにできるだけ早く届ける方法が書かれています。コードにバグがないかどうか、あるいは、本番環境へ失敗なくかつ素早くデプロイする方法など多岐にわたっています [#iikz2]_ 。
2012年に行われたカンファレンス、AWS re:Inventにて「Amazonは1時間に最大1000回もデプロイする」 [#iideploy]_ という公演がありました。
そのなかで、「Amazon.comでは11秒ごとに新しいコードがデプロイされている。そして最も多いときで1時間に1079回デプロイが行われた。
これには機能追加だけでなくバグフィクスなども含まれるが。平均で1万、最大で3万ものホストがデプロイを受け取る」とあります。
これは、バグはすぐに潰され、機能の追加の恩恵も受けられることを示します。このサイクルを行うために、継続的デリバリーでも強調されている **自動化** が必須となります。

例えば、この本の原稿の生成も自動化されています [#iikonohon]_ 。
githubにReST形式の原稿をpushすると、それを検知したjenkinsがsphinx [#iisphinx]_ のコマンドを実行し、入稿用のPDFが生成されます。

自動化の最先端として、githubにpull requestを行うとテストが実行され、そのあと本番環境へデプロイされる仕組みが@naoya氏のブログで紹介されています [#iighedep]_ 。
pull requiestをIRCなどのツールで自動化して作成し、Pull Request内容を確認、mergeするとそのままテストが走り、そして本番環境へコードが入ります。
自動化できるところは自動化しましょう。人的ミスがなくなります。

.. [#iikz] http://www.amazon.co.jp/dp/4048707876
.. [#iikz2] 自動化できるところは自動化しようぜ！って書いてあります。詳しくは読んでください
.. [#iideploy] http://www.publickey1.jp/blog/12/amazon11000_aws_reinventday2_am.html
.. [#iisphinx] ドキュメントビルダーのsphinxです。http://sphinx-users.jp/
.. [#iighedep] GitHub 時代のデプロイ戦略 http://d.hatena.ne.jp/naoya/20140502/1399027655
.. [#iikonohon] ななかInsidePRESS vol.1では原稿はGitHubにあり、PDFは手動でビルドしていました 
.. [#iivps] Virtual Private Server。仮想専用サーバのことです。この原稿PDFはさくらのVPSでビルドされています


Blue-Green Deployment
^^^^^^^^^^^^^^^^^^^^^^

本番環境に安全にデプロイするための方法です。「継続的デリバリー」にも載っている手法です。
本番環境といえば、ユーザがアクセスするサーバで、デプロイするためには、その本番環境のコードを変更することがあります。
さきほどの「まっしろわーるど」ではありませんが、一つ間違うと障害に直結します。本文から印象的な一文を引用してみましょう。

:: 
   万一問題が発生した場合にデプロイメントをロールバックできるようにしておくことが極めて重要だ。障害対応を稼働中の本番環境で進めようとすると、ほぼ間違いなく業務終了後の深夜作業となる。そしてミスを犯して残念な結果を招き、ユーザを怒らせることになるだろう。


このBlue-Green Deoploymentでは、BlueとGreenと呼ばれる2つの環境を用意します。ユーザからアクセスがある環境をBlue環境とします。
Green環境では、新しいバージョンのソフトウエアがデプロイされており、動作確認が終わったところです。
ユーザからのアクセスをルータによってBlueからGreenに変更することによって、デプロイを完了します。
もし、Green環境で問題が発生した場合、ルータの設定を変更してBlue環境にロールバックします。
こうして被害を最小限にしつつ、残った環境はステージングとしても使用することができます。
ただし、本番の環境が2つ必要になります [#iikanaria]_ 。このサーバを構築するために、IIは必須です。

[TODO]例の画像を突っ込んで解説

.. [#iikanaria] 一部のユーザを新しいバージョンをデプロイしたサーバに振り分けるカナリアリリースという手法も載っています。A/Bテストができたり、


テスト駆動インフラ
^^^^^^^^^^^^^^^^^

ソフトウエア界では、テスト駆動開発(TDD) [#iitdd]_ という言葉があります。つまり：

* テストを書いて、案の定失敗する
* テストが成功するコードを書く
* リファクタリングをする

というのが基本サイクルです [#iitdd2]_ 。「テスト駆動開発入門」という本がTDDの原典となっています。
まず動くコードを書いて、次にリファクタリングすることで、リファクタされたクリーンでかつ動くコードを作ることができます。テストをパスすることによって進捗が後戻りしないようになります。
コードが大きくなっても、テストにパスしなくてはならないため、バグが少ないコードになることが期待できます。

このテスト駆動開発を、インフラに応用するとどうなるでしょうか。
サーバの状態をチェックするテストを書くことから始まります。例えば、

* apacheがインストールされているかというテストを書きます。実行すると失敗します [#iitkf]_ 
* 何らかの方法でインストールします
* 再度テストを行い、テストが成功することを確認します

そのためにサーバの状態をチェックするためのserverepec [#iiserverspec]_ が登場しました。serverspecについては、後ほどインストールから使い方まで触れます。

.. [#iitdd] test-driven development
.. [#iitdd2] http://ja.wikipedia.org/wiki/インフラ駆動開発
.. [#iitkf] ミニマルでインストールしていたサーバだったとして、ここでは失敗することにしてください
.. [#iiserverspec] http://serverspec.org/


Immutable Infrastructure の利点
-------------------------------

自動化されるとどういうことが起きるかというと、仮想化技術を使って、壊して作りなおすことが簡単になります。自動化により、人の手による設定ミスや漏れがなくなります[特に本番環境に対して有効]。
これは、簡単にサーバを構築できるというインフラの側面だけでなく、ソフトウエアに対しても恩恵があります。
ソフトウエアのテストを行う場合を考えてみます。ひとつのサーバに開発環境が乗っかっている場合、ミドルウエアのバージョンは環境に固定されてしまいます。
このとき、新規にサーバを自動で構築してテストを行うことができるため、ミドルウエアのバージョンは自分で指定することが可能となります。

.. これやるとき、テスト書いてることが前提となっているの

.. herokuの具体例出したほうが早い？かなぁ


IIの三層
--------

「おーけすとれーしょん」「こんふぃぐれーしょん」「ぶーとすとらっぴんぐ」という三層の考え方があります [#iisansou]_ 。
どういう設定をどの層で行うかというのは、議論の余地があり、正確な定義はゆらいでいる状態です。

* Orchestration
  
  * Fabric, Capistrano, MCollective

* Configuration

  * Puppet, Chef, AWS OpsWorks

* Bootstrapping

  * Kickstart, Cobbler, OpenStack, AWS


.. [#iisansou] ひらがなで書いてあるのはなんでかって？その方がかわいいじゃないですか、だそうです(中の人談)[誰]


早速実践しよう
-------------

.. 何を目的としている？

テストを書けよ！
chef辛いという話を聞く。ansibleに鞍替えしてみたい誘惑に駆られる。
が、結局、構築したものがきちんと動いているかどうか確かめる必要がある。だから構築されているサーバに対してserverspecでテストを書くところから始めた。

ここからserverspecの実践を始める。

serverspec
-----------

serverspecとは
^^^^^^^^^^^^^^^

使ってみる
^^^^^^^^^^



docker
--------

dockerとは
^^^^^^^^^^^

使ってみる
^^^^^^^^^

vagrant
--------

vagrantとは
^^^^^^^^^^^

使ってみる
^^^^^^^^^^

ログの管理どうする？
------------------

fluentdを使って収集しましょう。いつでもサーバを壊せる状態にしておきましょう。
Elasticsearch + kibanaでログを可視化できてはっぴー☆

.. fluentdを使う利点とか書く。


DBどうするよ？
-------------

気軽に壊せないので、こわさない。以上！！

サーバの監視どうしよう
--------------------

気軽にこわせて気軽に立ち上がるサーバに名前をつけると大変なことに！！！
サーバに名前を付けることは悪であるという議論。hobbitとかzabbixとかそういうツールだと登録してるホストがなくなるとデータがなくなっちゃうんだよねー過去のトレンドが消えてしまうことが問題
mackerelを取り上げる。



とりまく技術
--------------------

* 概念

  * DevOps
  * 継続的デリバリー

   * 一日に何回デプロイしてますか？
 
  * II
  * blue-green
  * disposable
  * orchestration
  * test
  * 構成管理をcode化するということ

* 技術

  * chef
  * ansible
  * AWS
  * docker
  * vagrant
  * fluentd

    * ログをどうするかの話
    * じゃあDBどうすんのよ。頑張れ！！！

  * Serf
 
    * hostsを書き換える例
 
  * serverspec
  * mackerel.io
  * AMIをコピーするという運用

壮大なメモ
----------

* PhenixServer : http://martinfowler.com/bliki/PhoenixServer.html

  * フェニックスサーバ。認証監査をしようと思った
   * 今動いている本番環境を再度構築しなおすことになる
   * 定期的にサーバを焼き払ったほうがいい
   * サーバは灰の中から不死鳥のように蘇る。だからフェニックスサーバという
   * 構成のズレ、アドホックな変更でサーバの設定が漂流する。SnowflakeServersにいきつく
   * このような漂流に対向するためにpuppetやchefをつかってサーバを同期し直す。
   * netflixはランダムにサーバを落として大丈夫か試している（ひー

* SnowflakeServer : http://martinfowler.com/bliki/SnowflakeServer.html

　 * スノーフレークサーバ。雪のかけらサーバという存在
 　* OSやアプリケーションにパッチを当てたりする必要がある
　 * 設定を調査すると、サーバによって微妙に違う
　 * スキー場にとっては良いが、データセンターではよくない
　 * スノーフレークサーバは再現が難しい
　 * 本番での障害を開発環境で再現させても調査できない
　
     * 参考文献・目に見えるOpsハンドブック　http://www.amazon.com/gp/product/0975568604
   
   * 芸術家はスノーフレークを好むのだそうだ　http://tatiyants.com/devops-is-ruining-my-craft/
　
     * （サーバ含めそのなかのアプリケーションも工業製品なんだよ！！！わかったか！！！（横暴
     * （昔はひとつのサーバでなんとか出来たけど、今はアクセスも増えてサーバも増えたので芸術品はいらない！！
     * （どーどー落ち着けー、なーー
　
  * スノーフレークのディスクイメージを造ればいいじゃんという議論
  * だがこのディスクイメージはミスや不要な設定も一緒に入っている
  * しかもそれを変更することもある。壊れやすさの真の理由となる（雪だけに
  * 理解や修正がしにくくなる。変更したら影響がどこに及ぶかわからない
  * そんなわけで古代のOSの上に重要なソフトウエアが動作している理由である
  * スノーフレークを避けるためにはpuppetやchefを使って動作の確認のとれたサーバを保持すること
  * レシピを使用すつと、簡単に再構築できる。または、イメージデータを作れる
  * 構成はテキストファイルだから変更はバージョン管理される

  * nologinにしてchefなどからレシピを実行すれば、変更はすべてログに残り監査に対して有効
  * 構成の違いによるバグを減らし、全く同じ環境をつくれる。また、環境の違いに起因するバグを減らせる

    * 継続的デリバリーの本に言及する　あっ

* ConfigurationSynchronization : http://martinfowler.com/bliki/ConfigurationSynchronization.html

  * あんまり重要じゃない

* ImmutableServer : http://martinfowler.com/bliki/ImmutableServer.html

  * やっともどってこれた。この文章からスノーフレークとフェニックスサーバに飛んでいる
  * Netflixが実は実戦でやってたみたい　AMIつくってそれをAWS上に展開している

    * http://techblog.netflix.com/2013/03/ami-creation-with-aminator.html
    * AMIを作るツール　https://github.com/Netflix/aminator#readme


結論
-----

最適な技術をうまく組み合わせて事故のないデプロイをしていこう。


参考文献
--------
「継続的デリバリー 信頼できるソフトウェアリリースのためのビルド・テスト・デプロイメントの自動化」アスキー・メディアワークス,2012

